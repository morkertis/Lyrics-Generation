{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model 1\n",
    "The first architecture is based on language-model. <br>\n",
    "- words input - will receive a list of sequence consisting of 10 words, embed them and concatenate them with the melody input\n",
    "- melody input - will receive a vector of the summarized melody information, extracted from pretty_midi object <br>\n",
    "**Output** - probabilities vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\TomerMeirman\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Model1Base as mb\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lyrics_train_set.csv\",header=None)\n",
    "df = df.fillna('')\n",
    "df[2] = df[2] + df[3] + df[4] + df[5] + df[6] \n",
    "df=df.drop([3,4,5,6],axis=1)\n",
    "df.columns=['singer','song','lyrics']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text and adding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_lyrics'] = df.apply(lambda row: mb.clean_text(row.lyrics),axis=1)\n",
    "df['singer_song']= df.apply(lambda row: mb.clean_singer_song(row['singer'],row['song']),axis=1)\n",
    "tokenizer = RegexpTokenizer(r'\\w+|&+')\n",
    "df[\"tokens\"] = df[\"clean_lyrics\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a previously created midi vectors\n",
    "<br>*The function that creates a midi vector exists on Model1Base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_df = pd.read_pickle(\"data/melody_df.pkl\")\n",
    "df_concat=pd.merge(df,midi_df,how='inner', left_on='singer_song', right_on='filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176682 words total, with a vocabulary size of 7474\n",
      "Max sentence length is 1481\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in df_concat[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in df_concat[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "VALIDATION_SPLIT=.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating words sequences for input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7474 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df_concat[\"clean_lyrics\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(df_concat[\"clean_lyrics\"].tolist())\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Song sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 1760980\n"
     ]
    }
   ],
   "source": [
    "song_index=[]\n",
    "sequences_list=[]\n",
    "for i,seq in enumerate(sequences):\n",
    "    for j in range(1, len(seq)):\n",
    "        for z in range(MAX_SEQUENCE_LENGTH):\n",
    "            sequence = seq[j:j+z+2]\n",
    "            sequences_list.append(np.array(sequence))\n",
    "            song_index.append(i)\n",
    "print('Total Sequences: %d' % len(sequences_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding sequences according the max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_length = max([len(seq) for seq in sequences_list])\n",
    "sequences_pad = pad_sequences(sequences_list, maxlen=max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranging data for X_train and y_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(sequences_pad)\n",
    "song_index =np.array(song_index)\n",
    "X=data[:,:-1]\n",
    "Y=data[:,-1]\n",
    "midi_data = df_concat[[i for i in range(297)]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word2vec pretrained model to embed all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TomerMeirman\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7475, 300)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format('data/wiki-news-300d-1M.vec')\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.uniform(-1,1,EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, midi_train, y_train, x_test, midi_test, y_test = mb.create_training_data(song_index, midi_data, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "*The full function is in Model1Base.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mb.build_model(word_index, embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training** (first iteration example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0727 19:28:53.344172  5940 deprecation.py:323] From C:\\Users\\TomerMeirman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1734290 samples, validate on 26690 samples\n",
      "Epoch 1/10\n",
      "1734290/1734290 [==============================] - 566s 326us/step - loss: 5.3842 - acc: 0.1699 - val_loss: 6.1168 - val_acc: 0.1714\n",
      "Epoch 2/10\n",
      "1734290/1734290 [==============================] - 560s 323us/step - loss: 4.6387 - acc: 0.2066 - val_loss: 6.0950 - val_acc: 0.1743\n",
      "Epoch 3/10\n",
      "1734290/1734290 [==============================] - 561s 323us/step - loss: 4.1614 - acc: 0.2376 - val_loss: 6.2330 - val_acc: 0.1781\n",
      "Epoch 4/10\n",
      "1734290/1734290 [==============================] - 561s 323us/step - loss: 3.8197 - acc: 0.2706 - val_loss: 6.2501 - val_acc: 0.1741\n",
      "Epoch 5/10\n",
      "1734290/1734290 [==============================] - 562s 324us/step - loss: 3.5851 - acc: 0.3003 - val_loss: 6.3028 - val_acc: 0.1820\n",
      "Epoch 6/10\n",
      "1734290/1734290 [==============================] - 562s 324us/step - loss: 3.4021 - acc: 0.3241 - val_loss: 6.3285 - val_acc: 0.1869\n",
      "Epoch 7/10\n",
      "1734290/1734290 [==============================] - 562s 324us/step - loss: 3.2478 - acc: 0.3449 - val_loss: 6.3697 - val_acc: 0.1895\n",
      "Epoch 8/10\n",
      "1734290/1734290 [==============================] - 563s 324us/step - loss: 3.1138 - acc: 0.3635 - val_loss: 6.4349 - val_acc: 0.1863\n",
      "Epoch 9/10\n",
      "1734290/1734290 [==============================] - 564s 325us/step - loss: 2.9946 - acc: 0.3806 - val_loss: 6.4767 - val_acc: 0.1849\n",
      "Epoch 10/10\n",
      "1734290/1734290 [==============================] - 553s 319us/step - loss: 2.8876 - acc: 0.3961 - val_loss: 6.4931 - val_acc: 0.1830\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "        [x_train,midi_train],\n",
    "        y_train, \n",
    "        batch_size = 256, \n",
    "        epochs = 10, \n",
    "        validation_data=([x_test,midi_test], y_test))#,callbacks=[early_stopping_monitor,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "datestr=now.strftime(\"%Y_%m_%d__%H%M\")\n",
    "\n",
    "name='model_'+datestr\n",
    "mb.save_model(model,name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
