{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model 2\n",
    "The architecture of the second model is based on Neural Machine Translation model.<br>\n",
    "The encoder input will be the melody input and the output will be melody sequence, hidden state and cell state<br>\n",
    "The decoder input will be the encoder's output and the sequence input.\n",
    "The last layers are attention layer and softmax layer that outputs probabilities vector.\n",
    "- **melody input** - will receive a matrix consisting the information of chroma and piano, based on sampling each 1 second for 221 seconds (Median of all the melodies length) \n",
    "- **sequence input** - will receive a list of sequence consisting of 10 words, and embed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\mor\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import Model2Base as mb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading lyrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lyrics_train_set.csv\",header=None)\n",
    "df = df.fillna('')\n",
    "df[2] = df[2] + df[3] + df[4] + df[5] + df[6] \n",
    "df=df.drop([3,4,5,6],axis=1)\n",
    "df.columns=['singer','song','lyrics']\n",
    "\n",
    "df['song_num']=df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating midi matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mor\\Anaconda3\\lib\\site-packages\\pretty_midi\\pretty_midi.py:100: RuntimeWarning: Tempo, Key or Time signature change events found on non-zero tracks.  This is not a valid type 0 or type 1 MIDI file.  Tempo, Key or Time Signature may be wrong.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "midis_vec = mb.create_midis_vector(df)\n",
    "number_of_sequences = mb.get_median_length(midis_vec)\n",
    "songs_dict = {}\n",
    "for midi in midis_vec:\n",
    "    midi_file = midi[1]\n",
    "    mat = mb.create_midi_matrix(midi_file, number_of_sequences)\n",
    "    songs_dict[midi[0]] = mat\n",
    "    \n",
    "df=df[df.song_num.isin(songs_dict.keys())]\n",
    "df['clean_lyrics'] = df.apply(lambda row: mb.clean_text(row.lyrics),axis=1)\n",
    "df['singer_song']=df.apply(lambda row: mb.clean_singer_song(row['singer'],row['song']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"clean_lyrics\"].apply(tokenizer.tokenize)\n",
    "df['ln']= df[\"tokens\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180248 words total, with a vocabulary size of 7525\n",
      "Max sentence length is 1481\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in df[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in df[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))\n",
    "VOCAB_SIZE = len(VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(df[\"clean_lyrics\"].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_lyrics\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Song sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 1796570\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 10\n",
    "song_index=[]\n",
    "sequences_list=[]\n",
    "song_num=df.song_num.tolist()\n",
    "for i,seq in enumerate(sequences):\n",
    "    for j in range(1, len(seq)):\n",
    "        for z in range(MAX_SEQUENCE_LENGTH):\n",
    "            sequence = seq[j:j+z+2]\n",
    "            sequences_list.append(np.array(sequence))\n",
    "            song_index.append(song_num[i])\n",
    "print('Total Sequences: %d' % len(sequences_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequences according the max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = max([len(seq) for seq in sequences_list])\n",
    "sequences_pad = pad_sequences(sequences_list, maxlen=max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearranging data for X_train and y_train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(sequences_pad)\n",
    "song_index =np.array(song_index)\n",
    "X=data[:,:-1]\n",
    "Y=data[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the words for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('data/wiki-news-300d-1M.vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7526, 300)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM=300\n",
    "\n",
    "embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in word_index.items():\n",
    "    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.uniform(-1,1,EMBEDDING_DIM)\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list, midi_list, y_train_list, x_test_list, midi_test_list, y_test_list = mb.create_training_data(song_index, songs_dict, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.array(x_test_list)\n",
    "midi_test=np.array(midi_test_list)\n",
    "y_test=np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mb.build_model(word_index, embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "gen= mb.gendata(x_train_list,midi_list,y_train_list,batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Training** \n",
    "#### 5 epochs each time, batch size 256, data is matched by the generator (gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 22s 22s/step - loss: 5.6810 - acc: 0.1523 - val_loss: 6.4441 - val_acc: 0.1460\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(gen,\n",
    "                            steps_per_epoch=len(y_train_list)//batch_size,\n",
    "                            epochs=5,\n",
    "                            validation_data=([midi_test,x_test], y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
